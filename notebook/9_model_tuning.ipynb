{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 오늘 할 일\n",
    "### Model Tuning\n",
    "- 기본적인 방법론으로 best model 2~3개 tuning해서 test auc 0.001 이하로 차이나는 best 5 model 뽑기\n",
    "- 3가지 방법 시도\n",
    "    - prediction 끼리의 RMSE의 차이를 보면서 가장 맞추는 분야가 다른 2~3개로 ensemble\n",
    "    - 단순 5개의 average, weighted average 시도\n",
    "    - 5개 이상의 모델에 대해 meta learner를 학습하여 시도\n",
    "    \n",
    "### Feature Engineering\n",
    "- 더 뽑을 feature 2~3개만 더 찾아보고 결과 확인\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - FuzzyWuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzywuzzy\n",
    "import sys\n",
    "sys.path.append(\"/Users/angrypark/Desktop/korean-text-matching-tf/\")\n",
    "\n",
    "from utils.utils import JamoProcessor\n",
    "\n",
    "processor = JamoProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"../data/small/train_set.csv\")\n",
    "val_set = pd.read_csv(\"../data/small/val_set.csv\")\n",
    "test_set = pd.read_csv(\"../data/small/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/small/train.txt\", \"r\") as f:\n",
    "    train_A, train_B, train_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    train_labels = [1 if l==\"1\" else 0 for l in train_labels]\n",
    "with open(\"../data/small/val.txt\", \"r\") as f:\n",
    "    val_A, val_B, val_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    val_labels = [1 if l==\"1\" else 0 for l in val_labels]\n",
    "with open(\"../data/small/test.txt\", \"r\") as f:\n",
    "    test_A, test_B, test_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    test_labels = [1 if l==\"1\" else 0 for l in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab_probs</th>\n",
       "      <th>ba_probs</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>semantic_sim</th>\n",
       "      <th>substring_ratio</th>\n",
       "      <th>tfidf_char_sim</th>\n",
       "      <th>tfidf_word_sim</th>\n",
       "      <th>wm_distance</th>\n",
       "      <th>a_length</th>\n",
       "      <th>a_num_tokens</th>\n",
       "      <th>b_length</th>\n",
       "      <th>b_num_tokens</th>\n",
       "      <th>is_a_question</th>\n",
       "      <th>is_b_question</th>\n",
       "      <th>lengths_diff</th>\n",
       "      <th>min_length</th>\n",
       "      <th>token_overlap</th>\n",
       "      <th>count_word_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.962280</td>\n",
       "      <td>0.929438</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.838481</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.630266</td>\n",
       "      <td>0.230056</td>\n",
       "      <td>23.668947</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.960446</td>\n",
       "      <td>0.973614</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.841925</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.784759</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.950925</td>\n",
       "      <td>0.906347</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.795042</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.329870</td>\n",
       "      <td>0.066849</td>\n",
       "      <td>48.326650</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.678189</td>\n",
       "      <td>0.873261</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.862006</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.683584</td>\n",
       "      <td>0.093673</td>\n",
       "      <td>27.948451</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.691691</td>\n",
       "      <td>0.141754</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.577821</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.287779</td>\n",
       "      <td>0.045870</td>\n",
       "      <td>35.314967</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab_probs  ba_probs  edit_distance  semantic_sim  substring_ratio  \\\n",
       "0  0.962280  0.929438       0.352941      0.838481         0.352941   \n",
       "1  0.960446  0.973614       0.514286      0.841925         0.228571   \n",
       "2  0.950925  0.906347       0.628571      0.795042         0.142857   \n",
       "3  0.678189  0.873261       0.615385      0.862006         0.384615   \n",
       "4  0.691691  0.141754       0.520000      0.577821         0.240000   \n",
       "\n",
       "   tfidf_char_sim  tfidf_word_sim  wm_distance  a_length  a_num_tokens  \\\n",
       "0        0.630266        0.230056    23.668947         9             3   \n",
       "1        0.384922        0.000000    25.784759        16             4   \n",
       "2        0.329870        0.066849    48.326650        11             4   \n",
       "3        0.683584        0.093673    27.948451         7             3   \n",
       "4        0.287779        0.045870    35.314967        12             4   \n",
       "\n",
       "   b_length  b_num_tokens  is_a_question  is_b_question  lengths_diff  \\\n",
       "0         7             3              0              0             2   \n",
       "1        10             3              0              0             6   \n",
       "2        18             7              1              1             7   \n",
       "3         5             3              1              1             2   \n",
       "4         8             4              1              1             4   \n",
       "\n",
       "   min_length  token_overlap  count_word_sim  \n",
       "0           7              1        0.333333  \n",
       "1          10              1        0.000000  \n",
       "2          11              1        0.288675  \n",
       "3           5              1        0.500000  \n",
       "4           8              1        0.288675  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/small/train.txt\", \"r\") as f:\n",
    "    train_A, train_B, train_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    train_labels = [0 if l==\"0\" else 1 for l in train_labels]\n",
    "with open(\"../data/small/val.txt\", \"r\") as f:\n",
    "    val_A, val_B, val_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    val_labels = [0 if l==\"0\" else 1 for l in val_labels]\n",
    "with open(\"../data/small/test.txt\", \"r\") as f:\n",
    "    test_A, test_B, test_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    test_labels = [0 if l==\"0\" else 1 for l in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.85840112\n",
      "Val   : 0.74621097\n",
      "Test  : 0.73307931\n",
      "CPU times: user 23.9 s, sys: 130 ms, total: 24.1 s\n",
      "Wall time: 24.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(pd.concat([train_set, val_set]), train_labels + val_labels)\n",
    "print(\"Train Accuracy: {:.8f}\".format(xgb.score(train_set, train_labels)))\n",
    "probs = [p[1] for p in model.predict_proba(train_set)]\n",
    "print(\"Train ROAUC   : {:.8f}\".format(roc_auc_score(train_labels, probs)))\n",
    "print(\"Val   Accuracy: {:.8f}\".format(xgb.score(val_set, val_labels)))\n",
    "probs = [p[1] for p in model.predict_proba(val_set)]\n",
    "print(\"Val   ROAUC   : {:.8f}\".format(roc_auc_score(val_labels, probs)))\n",
    "print(\"Test  Accuracy: {:.8f}\".format(xgb.score(test_set, test_labels)))\n",
    "probs = [p[1] for p in model.predict_proba(test_set)]\n",
    "print(\"Test  ROAUC   : {:.8f}\".format(roc_auc_score(test_labels, probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_submission(model, name):\n",
    "    with open(\"../data/test_queries.txt\", \"r\") as f:\n",
    "        _, queries = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "        \n",
    "    with open(\"../data/test_replies.txt\", \"r\") as f:\n",
    "        reply_to_idx_dict = dict()\n",
    "        replies = list()\n",
    "        for line in f:\n",
    "            splits = line.strip().split(\"\\t\")\n",
    "            reply_to_idx_dict[splits[1]] = splits[0]\n",
    "            replies.append(splits[1])\n",
    "    \n",
    "    print(\"Train accuracy : {:.8f}\".format(model.score(train_set, train_labels)))\n",
    "    probs = [p[1] for p in model.predict_proba(train_set)]\n",
    "    print(\"Train ROAUC    : {:.8f}\".format(roc_auc_score(train_labels, probs)))\n",
    "    print(\"Val   accuracy : {:.8f}\".format(model.score(val_set, val_labels)))\n",
    "    probs = [p[1] for p in model.predict_proba(val_set)]\n",
    "    print(\"Val   ROAUC    : {:.8f}\".format(roc_auc_score(val_labels, probs)))\n",
    "    print(\"Test  accuracy : {:.8f}\".format(model.score(test_set, test_labels)))\n",
    "    probs = [p[1] for p in model.predict_proba(test_set)]\n",
    "    print(\"Test  ROAUC    : {:.8f}\".format(roc_auc_score(test_labels, probs)))\n",
    "    \n",
    "    length = len(replies)\n",
    "    submit_set = pd.read_csv(\"../data/submit_set.csv\")\n",
    "    features = list(submit_set.columns).copy()\n",
    "    features.remove(\"sentence_A\")\n",
    "    features.remove(\"sentence_B\")\n",
    "    \n",
    "    predictions = list()\n",
    "    for i, query in enumerate(queries):\n",
    "        data = submit_set.iloc[i*length:(i+1)*length]\n",
    "        probs = [p[1] for p in model.predict_proba(data[features]).tolist()]\n",
    "        scores = [(reply, score) for reply, score in zip(replies, probs)]\n",
    "        predict = sorted(scores, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        predictions.append(reply_to_idx_dict[predict])\n",
    "    submission = pd.read_csv(\"../submission/sample_submission.csv\")\n",
    "    submission[\"id_script\"] = pd.Series(predictions)\n",
    "    submission.to_csv(\"../submission/{}.csv\".format(name), index=False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 0.85840112\n",
      "Train ROAUC    : 0.93503186\n",
      "Val   accuracy : 0.74621097\n",
      "Val   ROAUC    : 0.83342753\n",
      "Test  accuracy : 0.73307931\n",
      "Test  ROAUC    : 0.81378481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "_ = model_to_submission(xgb, name=\"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz_simple_ratio = [fuzz.ratio(processor.word_to_jamo(a), processor.word_to_jamo(b)) \n",
    "                           for a, b in zip(train_A, train_B)] \n",
    "token_sort_ratio = [fuzz.token_sort_ratio(a, b) for a, b in zip(train_A, train_B)]\n",
    "train_set[\"fuzz_simple_ratio\"] = fuzz_simple_ratio\n",
    "train_set[\"fuzz_token_sort_ratio\"] = token_sort_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz_simple_ratio = [fuzz.ratio(processor.word_to_jamo(a), processor.word_to_jamo(b)) \n",
    "                           for a, b in zip(val_A, val_B)] \n",
    "token_sort_ratio = [fuzz.token_sort_ratio(a, b) for a, b in zip(val_A, val_B)]\n",
    "val_set[\"fuzz_simple_ratio\"] = fuzz_simple_ratio\n",
    "val_set[\"fuzz_token_sort_ratio\"] = token_sort_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz_simple_ratio = [fuzz.ratio(processor.word_to_jamo(a), processor.word_to_jamo(b)) \n",
    "                           for a, b in zip(test_A, test_B)] \n",
    "token_sort_ratio = [fuzz.token_sort_ratio(a, b) for a, b in zip(test_A, test_B)]\n",
    "test_set[\"fuzz_simple_ratio\"] = fuzz_simple_ratio\n",
    "test_set[\"fuzz_token_sort_ratio\"] = token_sort_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.85917391\n",
      "Train ROAUC   : 0.93523988\n",
      "Val   Accuracy: 0.74549893\n",
      "Val   ROAUC   : 0.83392629\n",
      "Test  Accuracy: 0.74257298\n",
      "Test  ROAUC   : 0.82238192\n",
      "CPU times: user 25.9 s, sys: 180 ms, total: 26.1 s\n",
      "Wall time: 26.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(pd.concat([train_set, val_set]), train_labels + val_labels)\n",
    "print(\"Train Accuracy: {:.8f}\".format(xgb.score(train_set, train_labels)))\n",
    "probs = [p[1] for p in xgb.predict_proba(train_set)]\n",
    "print(\"Train ROAUC   : {:.8f}\".format(roc_auc_score(train_labels, probs)))\n",
    "print(\"Val   Accuracy: {:.8f}\".format(xgb.score(val_set, val_labels)))\n",
    "probs = [p[1] for p in xgb.predict_proba(val_set)]\n",
    "print(\"Val   ROAUC   : {:.8f}\".format(roc_auc_score(val_labels, probs)))\n",
    "print(\"Test  Accuracy: {:.8f}\".format(xgb.score(test_set, test_labels)))\n",
    "probs = [p[1] for p in xgb.predict_proba(test_set)]\n",
    "print(\"Test  ROAUC   : {:.8f}\".format(roc_auc_score(test_labels, probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_set = pd.read_csv(\"../data/submit_set.csv\")\n",
    "with open(\"../data/submit_lines.txt\", \"r\") as f:\n",
    "    submit_A, submit_B = zip(*[line.strip().split(\"\\t\") for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz_simple_ratio = [fuzz.ratio(processor.word_to_jamo(a), processor.word_to_jamo(b)) \n",
    "                           for a, b in zip(submit_A, submit_B)] \n",
    "token_sort_ratio = [fuzz.token_sort_ratio(a, b) for a, b in zip(submit_A, submit_B)]\n",
    "submit_set[\"fuzz_simple_ratio\"] = fuzz_simple_ratio\n",
    "submit_set[\"fuzz_token_sort_ratio\"] = token_sort_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\"../data/small/train_set.csv\", index=False)\n",
    "val_set.to_csv(\"../data/small/val_set.csv\", index=False)\n",
    "test_set.to_csv(\"../data/small/test_set.csv\", index=False)\n",
    "submit_set.to_csv(\"../data/submit_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 0.85917391\n",
      "Train ROAUC    : 0.93523988\n",
      "Val   accuracy : 0.74549893\n",
      "Val   ROAUC    : 0.83392629\n",
      "Test  accuracy : 0.74257298\n",
      "Test  ROAUC    : 0.82238192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/angrypark/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "_ = model_to_submission(xgb, name=\"9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic_sim : 0.2542857229709625\n",
      "tfidf_char_sim : 0.14714285731315613\n",
      "ab_probs : 0.07571428269147873\n",
      "ba_probs : 0.07571428269147873\n",
      "min_length : 0.07571428269147873\n",
      "substring_ratio : 0.05000000074505806\n",
      "wm_distance : 0.05000000074505806\n",
      "fuzz_simple_ratio : 0.034285712987184525\n",
      "is_a_question : 0.03285714238882065\n",
      "is_b_question : 0.029999999329447746\n",
      "tfidf_word_sim : 0.025714285671710968\n",
      "b_num_tokens : 0.025714285671710968\n",
      "fuzz_token_sort_ratio : 0.02428571507334709\n",
      "a_length : 0.018571428954601288\n",
      "a_num_tokens : 0.018571428954601288\n",
      "b_length : 0.017142856493592262\n",
      "edit_distance : 0.015714285895228386\n",
      "token_overlap : 0.014285714365541935\n",
      "lengths_diff : 0.009999999776482582\n",
      "count_word_sim : 0.004285714123398066\n"
     ]
    }
   ],
   "source": [
    "feature_importance = sorted([(feat, imp) for feat, imp in zip(list(train_set.columns), xgb.feature_importances_)], key=lambda x: x[1], reverse=True)\n",
    "for feat, imp in feature_importance:\n",
    "    print(\"{} : {}\".format(feat, imp))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train Accuracy: 0.85917391\n",
    "Train ROAUC   : 0.93523988\n",
    "Val   Accuracy: 0.74549893\n",
    "Val   ROAUC   : 0.83392629\n",
    "Test  Accuracy: 0.74257298\n",
    "Test  ROAUC   : 0.82238192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085906\n",
      "Train Accuracy: 0.89946796\n",
      "Train ROAUC   : 0.96340044\n",
      "Val   Accuracy: 0.79859628\n",
      "Val   ROAUC   : 0.88574534\n",
      "Test  Accuracy: 0.71835443\n",
      "Test  ROAUC   : 0.80099261\n",
      "CPU times: user 3min 40s, sys: 14.5 s, total: 3min 55s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cb = CatBoostClassifier(verbose=False)\n",
    "cb.fit(pd.concat([train_set, val_set]), train_labels + val_labels)\n",
    "print(\"Train Accuracy: {:.8f}\".format(cb.score(train_set, train_labels)))\n",
    "probs = [p[1] for p in cb.predict_proba(train_set)]\n",
    "print(\"Train ROAUC   : {:.8f}\".format(roc_auc_score(train_labels, probs)))\n",
    "print(\"Val   Accuracy: {:.8f}\".format(cb.score(val_set, val_labels)))\n",
    "probs = [p[1] for p in cb.predict_proba(val_set)]\n",
    "print(\"Val   ROAUC   : {:.8f}\".format(roc_auc_score(val_labels, probs)))\n",
    "print(\"Test  Accuracy: {:.8f}\".format(cb.score(test_set, test_labels)))\n",
    "probs = [p[1] for p in cb.predict_proba(test_set)]\n",
    "print(\"Test  ROAUC   : {:.8f}\".format(roc_auc_score(test_labels, probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
