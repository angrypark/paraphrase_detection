{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load scripts and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hs/mudbox/ml/')\n",
    "from pingpong.utils import get_ingredient_factory\n",
    "factory = get_ingredient_factory()\n",
    "factory.config.add_property('corpora', 'preprocessed', False)\n",
    "pipeline = factory.get_preprocess_pipeline() + factory.get_tokenized_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLoader:\n",
    "    base_dir=''\n",
    "    \n",
    "    def __init__(self):        \n",
    "        self.preprocessor = Preprocessor()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load(filepath):\n",
    "        sent2id = {}\n",
    "        id2sent = []\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[1:]:\n",
    "                try:\n",
    "                    idx, sent = line.strip().split(',')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                \n",
    "                sent = pipeline.run(sent)\n",
    "                \n",
    "                sent2id[sent] = int(idx)\n",
    "                id2sent.append(sent)\n",
    "        \n",
    "        return sent2id, id2sent\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        query2id, id2query = cls._load(filepath=cls.base_dir + 'query_id.csv')\n",
    "        script2id, id2script = cls._load(filepath=cls.base_dir + 'script_id.csv')\n",
    "        return query2id, id2query, script2id, id2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2id, id2query, script2id, id2script = SentenceLoader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 200, 600, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query2id), len(script2id), len(id2query), len(id2script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hs/mudbox/ml/')\n",
    "from math import log10, sqrt\n",
    "from playground.hs_projects.resembla.feature_extraction.sentence_feature_extractor import TermVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sim:\n",
    "    \"\"\"Abstract class for cosine-based similarity\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def union(d1, d2):\n",
    "        \"\"\"두 딕셔너리의 키 값에서 차집합과 교집합을 구함\"\"\"\n",
    "        u1 = set(d1.keys())\n",
    "        u2 = set(d2.keys())\n",
    "        return u1 - u2, u1 & u2, u2 - u1\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(d):\n",
    "        \"\"\"the norm of vector\"\"\"\n",
    "        return sqrt(sum([i ** 2 for i in d.values()]))\n",
    "\n",
    "    @classmethod\n",
    "    def dict_similarity(cls, d1, d2):\n",
    "        norm_1 = cls.norm(d1)\n",
    "        norm_2 = cls.norm(d2)\n",
    "        _, intersection, _ = cls.union(d1, d2)\n",
    "\n",
    "        union_sum = sum([d1[word] * d2[word] for word in intersection])\n",
    "        return union_sum / (norm_1*norm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfModel:\n",
    "    def __init__(self):\n",
    "        idf = self.get_idf()\n",
    "        self.vectorizer = TermVectorizer(idf)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_idf():\n",
    "        with open('/media/scatter/projects/sandbox/project/AIChatbot/25000pingpong/pre_identified_corpus/document_frequency.txt', 'r') as f:\n",
    "            num_documents = int(f.readline().strip())\n",
    "            idf = {}\n",
    "            for line in f:\n",
    "                term, freq = line.strip().split('\\t')\n",
    "                idf[term] = log10(num_documents / int(freq))\n",
    "        return idf\n",
    "    \n",
    "    def preprocess(self, sent):\n",
    "        return self.vectorizer.vectorize(sent)['tfidf']\n",
    "        \n",
    "    def compare(self, pre_sent0, pre_sent1):\n",
    "        return Sim.dict_similarity(pre_sent0, pre_sent1)\n",
    "    \n",
    "    def score(self, sent0, sent1):\n",
    "        sent0 = self.preprocess(sent0)\n",
    "        sent1 = self.preprocess(sent1)\n",
    "        return self.compare(sent0, sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference\n",
    "- 만든 모델을 이용하여 inference합니다.\n",
    "- model의 interface는 다음과 같다고 가정합니다.  \n",
    "    - pair단위로 inference  \n",
    "        - score: query와 script pair의 score를 계산합니다.\n",
    "    - preprocess 후 사용\n",
    "        - preprocess: sentence를 preprocess 합니다.\n",
    "        - compare: preprocessed_query와 preprocessed_script의 score를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    \n",
    "    def __init__(self, query2id, id2query, script2id, id2script):\n",
    "        self.query2id = query2id\n",
    "        self.id2query = id2query\n",
    "        self.script2id = script2id\n",
    "        self.id2script = id2script\n",
    "    \n",
    "    def infer_pbp(self, model, filepath, verbose=0):\n",
    "        \"\"\"script와 query를 pair단위로 비교합니다.\"\"\"\n",
    "        results = []\n",
    "        times = []\n",
    "        for query in self.id2query:\n",
    "            _results = []\n",
    "            for script in self.id2script:\n",
    "                _start = time()\n",
    "                _results.append(model.score(query, script))\n",
    "                times.append(time() - _start)\n",
    "            results.append(_results)\n",
    "        results = np.argmax(np.array(results), axis=1)\n",
    "        \n",
    "        times = np.array(times)\n",
    "        mean_time = np.mean(times) * 1e6\n",
    "        std_time = np.std(times) * 1e6\n",
    "        print('%.4f ± %.4f μs per comparison' % (mean_time, std_time))\n",
    "        \n",
    "        if verbose:\n",
    "            print(self._verbose(results))\n",
    "        \n",
    "        self._save(filepath, results)\n",
    "        \n",
    "    def infer_preprocessed(self, model, filepath, verbose=0):\n",
    "        \"\"\"script와 query를 preprocess한 다음에 비교합니다.\"\"\"\n",
    "        \n",
    "        def _preprocess(model, sents):\n",
    "            _preprocessed = []\n",
    "            _times = []\n",
    "            for sent in sents:\n",
    "                _start = time()\n",
    "                _preprocessed.append(model.preprocess(sent))\n",
    "                _times.append(time() - _start)\n",
    "            return _preprocessed, _times\n",
    "        \n",
    "        preprocessed_id2query, query_preprosessing_time = _preprocess(model, self.id2query)\n",
    "        preprocessed_id2script, script_preprocessing_time = _preprocess(model, self.id2script)\n",
    "        \n",
    "        results = []\n",
    "        comparison_times = []\n",
    "        for query in preprocessed_id2query:\n",
    "            _results = []\n",
    "            for script in preprocessed_id2script:\n",
    "                _start = time()\n",
    "                _results.append(model.compare(query, script))\n",
    "                comparison_times.append(time() - _start)\n",
    "            results.append(_results)\n",
    "        results = np.argmax(np.array(results), axis=1)\n",
    "        \n",
    "        preprocessing_times = np.array(query_preprosessing_time + script_preprocessing_time)\n",
    "        mean_preprocessing_times = np.mean(preprocessing_times) * 1e6\n",
    "        std_preprocessing_times = np.std(preprocessing_times) * 1e6\n",
    "        print('%.4f ± %.4f μs per preprocessing' % (mean_preprocessing_times, std_preprocessing_times))\n",
    "        \n",
    "        comparison_times = np.array(comparison_times)\n",
    "        mean_comparison_times = np.mean(comparison_times) * 1e6\n",
    "        std_comparison_times = np.std(comparison_times) * 1e6\n",
    "        print('%.4f ± %.4f μs per comparison' % (mean_comparison_times, std_comparison_times))\n",
    "        \n",
    "        if verbose:\n",
    "            print(self._verbose(results))\n",
    "        \n",
    "        self._save(filepath, results)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save(filepath, results):\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write('id_query,id_script\\n')\n",
    "            for n, i in enumerate(results):\n",
    "                f.write('%d\\t,%d\\n' % (n, i))\n",
    "    \n",
    "    def _verbose(self, results):\n",
    "        eval_results = []\n",
    "        for query, result in zip(self.id2query, results):\n",
    "            eval_results.append((query, self.id2script[result]))\n",
    "        series = pd.Series(eval_results)\n",
    "        return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferrer = Inference(query2id, id2query, script2id, id2script)\n",
    "tfidf_model = TfidfModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5415 ± 3.8397 μs per preprocessing\n",
      "4.6463 ± 1.5086 μs per comparison\n"
     ]
    }
   ],
   "source": [
    "inferrer.infer_preprocessed(tfidf_model, 'submission_baseline.csv', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.1546 ± 4.0037 μs per comparison\n"
     ]
    }
   ],
   "source": [
    "inferrer.infer_pbp(tfidf_model, 'submission_baseline.csv', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
