{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on Quora Winners\n",
    "- Based on DeepParaphrase paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Based on Quora Winners \n",
    "- 그냥 이것만 보자 https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "- 회사에서 구현할 것! Word Mover's Distance : https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n",
    "- http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "sys.path.append(\"/home/angrypark/korean-text-matching-tf/\")\n",
    "\n",
    "from utils.utils import JamoProcessor\n",
    "from text.tokenizers import SentencePieceTokenizer\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.sent_piece_model = \"/media/scatter/scatterdisk/tokenizer/sent_piece.100K.model\"\n",
    "config = Config()\n",
    "\n",
    "processor = JamoProcessor()\n",
    "tokenizer = SentencePieceTokenizer(config)\n",
    "def my_word_tokenizer(raw, pos=[\"Noun\", \"Alpha\", \"Verb\", \"Number\"], stopword=[]):\n",
    "    return [word for word in tokenizer.tokenize(raw)]\n",
    "\n",
    "def my_char_tokenizer(raw, pos=[\"Noun\", \"Alpha\", \"Verb\", \"Number\"], stopword=[]):\n",
    "    return [processor.word_to_jamo(word) for word in tokenizer.tokenize(raw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### TFIDF Vectorizer + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_word_vectorizer = pickle.load(open(\"../dump/tfidf_word_vectorizer_big.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 5min 28s, total: 5min 40s\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"../data/small/train.txt\", \"r\") as f:\n",
    "    # read raw text lines\n",
    "    train_A, train_B, train_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    train_labels = [1 if l==\"1\" else 0 for l in train_labels] \n",
    "    \n",
    "    # make tfidf vectorized dataset\n",
    "    train_set = tfidf_word_vectorizer.transform(train_A).toarray()\n",
    "    train_set = np.concatenate((train_set, tfidf_word_vectorizer.transform(train_B).toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 s, sys: 452 ms, total: 21.6 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8218587247646129"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 884 ms, sys: 440 ms, total: 1.32 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"../data/small/val.txt\", \"r\") as f:\n",
    "    # read raw text lines\n",
    "    val_A, val_B, val_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    val_labels = [1 if l==\"1\" else 0 for l in val_labels] \n",
    "    \n",
    "    # make tfidf vectorized dataset\n",
    "    val_set = tfidf_word_vectorizer.transform(val_A).toarray()\n",
    "    val_set = np.concatenate((val_set, tfidf_word_vectorizer.transform(val_B).toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6231817719458854"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(val_set, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 720 ms, sys: 332 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"../data/small/test.txt\", \"r\") as f:\n",
    "    # read raw text lines\n",
    "    test_A, test_B, test_labels = zip(*[line.strip().split(\"\\t\") for line in f])\n",
    "    test_labels = [1 if l==\"1\" else 0 for l in test_labels] \n",
    "    \n",
    "    # make tfidf vectorized dataset\n",
    "    test_set = tfidf_word_vectorizer.transform(test_A).toarray()\n",
    "    test_set = np.concatenate((test_set, tfidf_word_vectorizer.transform(test_B).toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6565486954275381"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_set, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature로 쓸만함. models에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb, open(\"../models/nb.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer + SVD + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52min 57s, sys: 25min 31s, total: 1h 18min 28s\n",
      "Wall time: 9min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svd = TruncatedSVD(n_components=150)\n",
    "svd.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_svd = svd.transform(train_set)\n",
    "val_set_svd = svd.transform(val_set)\n",
    "test_set_svd = svd.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/small/train_tfidf_vec.npy\", train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/small/val_tfidf_vec.npy\", val_set)\n",
    "np.save(\"../data/small/test_tfidf_vec.npy\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_set_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_svd_scaled = scaler.transform(train_set_svd)\n",
    "val_set_svd_scaled = scaler.transform(val_set_svd)\n",
    "test_set_svd_scaled = scaler.transform(test_set_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(C=1.0, probability=True)\n",
    "svm.fit(train_set_svd_scaled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
