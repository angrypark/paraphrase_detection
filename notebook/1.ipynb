{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "---\n",
    "### 오늘 할 일\n",
    "- 데이터를 뿔리자\n",
    "- feature 뽑는 코드 작성\n",
    "- baseline score 내보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive 기준\n",
    "- 직접 train에 사용하지 않을 2등 모델의 semantic score가 0.9 이상\n",
    "- edit distance가 일정 수준 이하\n",
    "- token 차이가 일정 수준 이하\n",
    "- tokenizer는 soynlp, sentencepiece 둘다 시도\n",
    "- 시제가 일정해야 함\n",
    "- 주체가 일정해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/angrypark/korean-text-matching-tf/\")\n",
    "\n",
    "from text.tokenizers import JamoTokenizer, TwitterTokenizer, SoyNLPTokenizer, SentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.sent_piece_model = \"/media/scatter/scatterdisk/tokenizer/sent_piece.100K.model\"\n",
    "        self.soynlp_scores = \"/media/scatter/scatterdisk/tokenizer/soynlp_scores.sol.100M.txt\"\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceTokenizer(config)\n",
    "# tokenizer = SoyNLPTokenizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/test_queries.txt\", \"r\") as f:\n",
    "    candidates = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_candidates = list()\n",
    "for q in candidates:\n",
    "    tokenized = tokenizer.tokenize(q)\n",
    "    tokenized_candidates.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.39"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(q) for q in tokenized_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter([len(q) for q in tokenized_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 9, 2: 45, 3: 61, 4: 54, 5: 16, 6: 7, 7: 6, 8: 2})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editdistance.eval(\"안녕\", \"안녕하\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3], [2,3,4], [3,4,5]])\n",
    "a[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import JamoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = JamoProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = list()\n",
    "with open(\"../data/small/train.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        splited = line.strip().split(\"\\t\")\n",
    "        if splited[2]==\"1\":\n",
    "            a = processor.word_to_jamo(splited[0]).replace(\"_\", \"\")\n",
    "            b = processor.word_to_jamo(splited[1]).replace(\"_\", \"\")\n",
    "            count.append(editdistance.eval(a, b))\n",
    "count = Counter(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 654,\n",
       "         2: 2058,\n",
       "         3: 3514,\n",
       "         4: 4726,\n",
       "         5: 6178,\n",
       "         6: 7164,\n",
       "         7: 6926,\n",
       "         8: 7278,\n",
       "         9: 6676,\n",
       "         10: 6170,\n",
       "         11: 5304,\n",
       "         12: 4378,\n",
       "         13: 3592,\n",
       "         14: 3000,\n",
       "         15: 2414,\n",
       "         16: 1958,\n",
       "         17: 1388,\n",
       "         18: 1062,\n",
       "         19: 848,\n",
       "         20: 678,\n",
       "         21: 528,\n",
       "         22: 340,\n",
       "         23: 286,\n",
       "         24: 188,\n",
       "         25: 126,\n",
       "         26: 108,\n",
       "         27: 58,\n",
       "         28: 60,\n",
       "         29: 32,\n",
       "         30: 36,\n",
       "         31: 8,\n",
       "         32: 4,\n",
       "         33: 14,\n",
       "         35: 2})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = None\n",
    "    \n",
    "    def extract_features(self, A, B, \n",
    "                         get_tfidf_similarity=True, \n",
    "                         get_edit_distance=True, \n",
    "                         get_negative_token_diff=True, ):\n",
    "        extracted_features = list()\n",
    "        extracted_features.append()\n",
    "        \n",
    "    def token_length(sentence):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(query, reply):\n",
    "    def token_length(sentence):\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        return len(tokenized)\n",
    "    \n",
    "    def sentence_length(sentence):\n",
    "        return len(sentence)\n",
    "    \n",
    "    def tfidf_similarity(sentence):\n",
    "        \n",
    "    def edit_distance(query, reply):\n",
    "        \n",
    "    def has_negative_token(sentence):\n",
    "        \n",
    "    def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
